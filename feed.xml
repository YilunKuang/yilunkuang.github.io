<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yilunkuang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yilunkuang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-01T23:50:27+00:00</updated><id>https://yilunkuang.github.io/feed.xml</id><title type="html">Yilun Kuang</title><subtitle>Yilun Kuang, Machine Learning, PhD, NYU, AI </subtitle><entry><title type="html">Rectified LpJEPA</title><link href="https://yilunkuang.github.io/blog/2026/rectified-lp-jepa/" rel="alternate" type="text/html" title="Rectified LpJEPA"/><published>2026-01-30T00:00:00+00:00</published><updated>2026-01-30T00:00:00+00:00</updated><id>https://yilunkuang.github.io/blog/2026/rectified-lp-jepa</id><content type="html" xml:base="https://yilunkuang.github.io/blog/2026/rectified-lp-jepa/"><![CDATA[<h2 id="self-supervised-learning">Self-Supervised Learning</h2> <p>Consider an input vector $\mathbf{x}$, such as an image, an audio clip, or a video frame. We would like to learn a neural network representation</p> \[\begin{align} \mathbf{z}=f_{\boldsymbol{\theta}}(\mathbf{x})\in\mathbb{R}^d \end{align}\] <p>in the absence of any labeling information. Self-supervised learning makes this possible by creating supervision from the data itself <d-cite key="balestriero2023cookbookselfsupervisedlearning"></d-cite>. Given $\mathbf{x}$, we can generate another view $\mathbf{x}'$ of the input $\mathbf{x}$ that preserves the same semantic content.</p> <ul> <li>For images, $\mathbf{x}'$ might be a cropped, rotated, or even corrupted version of $\mathbf{x}$</li> <li>For audio or video, $\mathbf{x}'$ can be a nearby time segment or adjacent frame.</li> </ul> <p>Although $\mathbf{x}$ and $\mathbf{x}'$ may look different, they are assumed to be semantically related. Thus we can learn a neural network representation by relying on the following principle: <strong>representations of different views of the same input should be similar</strong>. Translating to mathematical language, this means we can minimize the $\ell_2$ distance between the embeddings of different views</p> \[\begin{align} \min_{\boldsymbol{\theta}}\|\mathbf{z}-\mathbf{z}'\|_2 \tag{2} \end{align}\] <p>where \(\mathbf{z}' = f_{\boldsymbol{\theta}}(\mathbf{x}')\). By enforcing agreement across many randomly generated views, the network learns features that are invariant to nuisance transformations and capture meaningful structure in the data.</p> <h2 id="distribution-matching-regularization">Distribution-Matching Regularization</h2> <p>Simply minimizing the \(\ell_2\) distance (Eq. (2)) between views, however, can lead to the problem of <strong>feature collapse</strong>. In the extreme case, the network can map every input to the same vector, resulting in <strong>complete collapse</strong>. Eq. (2) is perfectly minimized, but the representation is useless as it cannot distinguish between different inputs. More subtle forms of collapse also occur, such as <strong>dimensional collapse</strong>, where different feature dimensions encode redundant information <d-cite key="jing2022understandingdimensionalcollapsecontrastive"></d-cite>.</p> <p>The goal of self-supervised learning is therefore to enforce invariance across views while maximally spreading feature vectors in the ambient space. One effective way to do this is to regularize the feature distributions \(\mathbb{P}_{\mathbf{z}}\) and \(\mathbb{P}_{\mathbf{z}'}\) towards a carefully chosen <strong>target distribution</strong> \(Q\), which explicitly encodes desirable properties such as dispersion and diversity across feature dimensions. Thus the self-supervised learning objective in Eq. (2) can be augmented as</p> \[\begin{align} \min_{\boldsymbol{\theta}}\|\mathbf{z}-\mathbf{z}'\|_2+\mathcal{L}(\mathbb{P}_{\mathbf{z}}\|Q)+\mathcal{L}(\mathbb{P}_{\mathbf{z}'}\|Q) \tag{3} \end{align}\] <p>where \(\mathcal{L}(\cdot\|\cdot)\) is any distribution-matching loss that’s minimized when the two distributions are identical.</p> <p>Naively, one can consider the KL divergence \(D_{\operatorname{KL}}(\mathbb{P}_{\mathbf{z}}\|Q)\) with the Monte-Carlo estimate: \(\begin{align} D_{\operatorname{KL}}(\mathbb{P}_{\mathbf{z}}\|Q) = \int\log\frac{d\mathbb{P}_{\mathbf{z}}(x)}{dQ(x)}d\mathbb{P}_{\mathbf{z}}(x)\approx \frac{1}{B}\sum_{i=1}^{B}\log\frac{p_{\mathbf{z}}(\mathbf{z}_i)}{q(\mathbf{z}_i)} \end{align}\) However, directly performing distribution-matching in high dimensional space suffers from the <strong>curse of dimensionality</strong>: density estimations are intractable and we require exponential number of samples in dimensions. Thus we resort to a family of method based on the Cramer-Wold theorem <d-cite key="cramer1936"></d-cite> <d-cite key="wold1938"></d-cite>.</p> <p>The <strong>Cramér–Wold theorem</strong> states that two random vectors \(\mathbf{x},\mathbf{y}\in\mathbb{R}^d\) are equal in distribution if and only if all their one-dimensional linear projections are equal in distribution.</p> \[\begin{align} \mathbf{x}\stackrel{\operatorname{d}}{=}\mathbf{y}\iff \mathbf{c}^\top\mathbf{x}\stackrel{\operatorname{d}}{=}\mathbf{c}^\top\mathbf{y}\text{ for all }\mathbf{c}\in\mathbb{R}^d \end{align}\] <p>where the superscript \(\operatorname{d}\) above the equal sign denotes equality in distribution. This result enables us to decompose a high-dimensional distribution matching problem into parallelized one-dimension optimizations under many different projections induced by \(\mathbf{c}\), which significantly reduces the sample complexity in each of the one-dimensional problems.</p> <p>With the Cramér–Wold theorem, Eq. (3) can be updated as</p> \[\begin{align} \min_{\boldsymbol{\theta}}\|\mathbf{z}-\mathbf{z}'\|_2+\mathbb{E}_{\mathbf{c}}[\mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})]+\mathbb{E}_{\mathbf{c}}[\mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}'}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})] \tag{4} \end{align}\] <p>where \(\mathbf{y}\sim Q\) and \(\mathbf{c}^\top\mathbf{y}\sim\mathbb{P}_{\mathbf{c}^\top\mathbf{y}}\) denotes the distribution of the projected targets. Thus we have converted a high-dimensional distribution matching problem into an expectation over univariate distribution-matching objectives. Even if Cramér–Wold theorem guarantees convergence under asymptotic number of projection vectors, in practice we only need finite projections and LeJEPA <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite> further shows that we only need to sample the projection vectors \(\mathbf{c}\) from the unit \(\ell_2\) sphere \(\mathbb{S}^{d-1}_{\ell_2}:=\{\mathbf{x}\in\mathbb{R}^{d}\mid\|\mathbf{x}\|_2=1\}\).</p> <p>In the next section, we discuss what are the choices of \(Q\) which encourages maximally spread-out and diverse features.</p> <h2 id="target-distributions">Target Distributions</h2> <h3 id="isotropic-gaussian-distributions">Isotropic Gaussian Distributions</h3> <p>One natural target distribtuion is the <strong>isotropic Gaussian</strong> \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\), which is used in LeJEPA <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite>.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-isotropic-gaussian">Probability Density Functions of Isotropic Gaussian</h5> \[p(\mathbf{z})=\frac{1}{(2\pi)^{d/2}}\exp\bigg(-\frac{1}{2}\|\mathbf{z}\|_2^2\bigg)\] </blockquote> <p>The choice of Gaussian as the target distribution is not an arbitrary one. Geometrically, a $d$-dimensional isotropic Gaussian random vector $\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})$ concentrates on a thin shell of radius $\sqrt{d}$ with an $O(1)$ width. Under polar decompositions, the radius $||\mathbf{z}||_2\sim\chi(d)$ and the angular direction $\mathbf{z}/||\mathbf{z}||_2$ are independent, with the direction uniformly distributed over the unit $\ell_2$ sphere with respect to the surface (Hausdorff) measure. This <strong>uniform angular distribution</strong> coincides exactly with the uniformity condition identified as optimal for contrastive learning <d-cite key="wang2022understandingcontrastiverepresentationlearning"></d-cite>. As a result, isotropic Gaussian features are maximally spread out in direction, providing a principled way to prevent feature collapse.</p> <div class="row justify-content-center" style="margin-top: 0.5rem;"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L2_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" style="margin-top: 0.15rem; margin-bottom: 0.5rem;"> Unit $\ell_2$ Sphere </div> <p>This geometric behavior has a direct information-theoretic correspondence. Among all possible probability distributions with a fixed expected $\ell_2$-norm (i.e., fixed average energy), the isotropic Gaussian <strong>maximizes entropy</strong>. In other words, if we constrain only how much energy the features carry and impose no further structure, the Gaussian is the most “spread out” distribution possible.</p> <details><summary>Details on Maximum-Entropy Distributions</summary> <p>TODO.</p> </details> <h3 id="product-laplace-distributions">Product Laplace Distributions</h3> <p>While isotropic Gaussian regularization effectively prevents feature collapse, it inherently favors dense representations, where most feature dimensions are active. In contrast, extensive evidence from neuroscience, signal processing, and machine learning suggests that <strong>sparse representations</strong> are often more efficient and robust. Sparse coding plays a central role in compressed sensing and robust recovery, and biological neural systems are known to encode sensory inputs using non-negative, sparse activations under metabolic constraints. <span style="color:red;">TODO: add citations later.</span></p> <p>Motivated by these observations, we seek to induce sparsity directly at the level of the feature distribution. A simple and principled approach is to replace isotropic Gaussian regularization with <strong>product Laplace</strong> \(\prod_{i=1}^{d}\mathcal{L}(0,\sigma)\) regularization.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-product-laplace">Probability Density Functions of Product Laplace</h5> \[p(\mathbf{z})=\frac{1}{(2\sigma)^d}\exp\bigg(-\frac{\|\mathbf{z}\|_1}{\sigma}\bigg)\] </blockquote> <p>Contrary to Gaussian, the product Laplace distribution \(\mathbf{z}\sim\prod_{i=1}^{d}\mathcal{L}(0,\sigma)\) is the <strong>maximum-entropy</strong> distribution under a fixed expected $\ell_1$-norm constraint. Its radius follows the Gamma distribution \(\|\mathbf{z}\|_1\sim\Gamma(d/1, 1)\) and the angular direction is <strong>uniform</strong> over the unit $\ell_1$ sphere.</p> <div class="row justify-content-center" style="margin-top: 0.5rem;"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L1_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" style="margin-top: 0.15rem; margin-bottom: 0.5rem;"> Unit $\ell_1$ Sphere </div> <p>The geometry of the \(\ell_1\) norm directly explains why the Product Laplace distribution induces sparsity. Unlike the smooth \(\ell_2\) sphere, the \(\ell_1\) geometry has sharp corners along coordinate axes, biasing samples toward configurations where many coordinates are small or close to zero. Thus regularizing feature distributions towards product Laplace lead to axis-aligned, sparse representations, while also encourages maximum spreading and hence prevent feature collapse.</p> <p>The other way to think about why Laplace induces sparsity is through the lens of regularized linear regression. It’s well known that <strong>Lasso regression</strong> with $\ell_1$ penalty on the weight is equivalent to Maximum A Posteriori (MAP) estimation with a <strong>Laplace prior</strong>, whereas <strong>Ridge regression</strong> with $\ell_2$ regularization on the weight corresponds to MAP estimation with a <strong>Gaussian prior</strong> <d-cite key="bishop2006pattern"></d-cite>. Hence just as how the Lasso loss constrains the \(\ell_1\) norm of the parameters, imposing Laplace distributions over features regularizes the output of the neural network functions and thus naturally induces sparsity in the learned representations.</p> <details><summary>Details on Regularized Linear Regression</summary> <p>TODO.</p> </details> <h3 id="generalized-gaussian-distributions">Generalized Gaussian Distributions</h3> <p>We observe that both Laplace and Gaussian are maximum-entropy distributions over either expected \(\ell_1\) amd \(\ell_2\) norm constraints. Since the \(\ell_1\)-norm already promotes sparsity, a natural question is how much further can we go.</p> <p>To answer this, we need to define our sparsity metrics. The most direct notion of sparsity is the <strong>$\ell_0$ (pseudo-)norm</strong> which simply counts the number of nonzero entries in a vector. The <strong>$\ell_1$-norm</strong> is commonly used as a convex surrogate for \(\ell_0\), but it still penalizes all nonzero entries linearly and can produce only approximately sparse solutions.</p> <p>More generally, $\ell_p$ <strong>quasi-norms</strong> with $0 &lt; p &lt; 1$ provide a closer approximation to $\ell_0$. Their sharp singularity near zero strongly encourages exact sparsity, while their weaker growth for large values reduces shrinkage on important components. Although nonconvex, such penalties are well known to yield significantly sparser representations than $\ell_1$ in practice. <span style="color:red;">TODO: add citations later.</span></p> <p>Thus we would like to consider distributions with the \(\ell_p\) quasi-norms constraints. In fact, the <strong>maximum-entropy</strong> distribution under the expected \(\ell_p\)-norm constraints is the zero-mean <strong>product Generalized Gaussian</strong> distributions \(\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\), of which product Laplace and isotropic Gaussian are special cases for \(p=1\) and \(p=2\) respectively.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-product-generalized-gaussian">Probability Density Functions of Product Generalized Gaussian</h5> \[p(\mathbf{z})=\frac{p^{d-d/p}}{(2\sigma)^d\Gamma(1/p)^d}\exp\bigg(-\frac{\|\mathbf{z}-\boldsymbol{\mu}\|_p^p}{p\sigma^p}\bigg)\] </blockquote> <p>Assume that $\mu=0$ and let \(\mathbf{z}\sim\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\). Then the radius \(r^p:=\|\mathbf{z}\|_p^p\sim\Gamma(d/p,p\sigma^p)\) follows the Gamma distribution and the angular direction \(\mathbf{u}:=\mathbf{z}/\|\mathbf{z}\|_p\) follows the cone measure on the $\ell_p$ sphere \(\mathbb{S}^{d-1}_{\ell_{p}}:=\{\mathbf{z}\in\mathbb{R}^d\mid\|\mathbf{z}\|_p=1\}\) with the radial-angular independence \(r\perp \!\!\ \mathbf{u}\) <d-cite key="barthe2005probabilistic"></d-cite>.</p> <details><summary>Details on Cone and Surface Measure</summary> <p>The cone measure is identical to the \((d-1)\)-dimensional Hausdorff measure \(\mathcal{H}^{d-1}\) (also called surface measure) when \(p\in\{1,2,\infty\}\) <d-cite key="alonso2019gaussian"></d-cite>. By definition, if \(A\subseteq \mathbb{S}^{d-1}_{\ell_{p}}\), then \(p(\mathbf{u}\in A)=\mathcal{H}^{d-1}(A)/\mathcal{H}^{d-1}(\mathbb{S}^{d-1}_{\ell_{p}})\).</p> <p>Thus the angular directions of product Laplace and isotropic Gaussian are uniformly distributed over the \(\ell_p\) sphere with respect to the surface measures, whereas any other Generalized Gaussian distributions have angular direction uniform under the cone measure.</p> </details> <div class="row mt-3 text-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.75_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.75}$ sphere </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.50_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.5}$ sphere </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.25_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.25}$ sphere </div> </div> </div> <p>Thus we can always regularizes our feature distributions towards the Generalized Gaussian Distributions \(\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\) with \(0&lt;p&lt;1\) for learning even sparser, axis-aligned representations while also preserving the maximum-entropy guarantee to prevent feature collapse.</p> <h3 id="rectified-generalized-gaussian-distributions">Rectified Generalized Gaussian Distributions</h3> <p>The Generalized Gaussian family is a well-known distribution, but we’re not satisfied with the \(\ell_p\)-norm sparsity it induces. In fact, it’s possible to directly encode \(\ell_0\)-norm into the target distribution, and this brings us to the key innovation of our paper: <strong>regularizing rectified features towards the Rectified Generalized Gaussian distributions</strong>.</p> <p>Let \(\mathbf{x}\sim\prod_{i=1}^d\mathcal{GN}_p(\mu,\sigma)\) be a Generalized Gaussian random vector. Then we can obtain the (product) Rectified Generalized Gaussian random vector as \(\mathbf{z}\sim\prod_{i=1}^d\operatorname{ReLU}(\mathcal{GN}_p(\mu,\sigma))\), where we apply the rectifying nonlinearities coordinate-wise to the Generalized Gaussian random vector. We visualize the samples drawn from the Generalized Gaussian and Rectified Generalized Gaussian distribution in \(2\)-dimensions when \(p=2\).</p> <div class="row mt-3 text-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Samples from Gaussian and Rectified Gaussian </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> ReLU sqaushes the Gaussian samples into the axis. </div> </div> </div> <p>As illustrated in the figure above, rectification collapses all samples lying outside the positive orthant onto its boundary, while samples in the interior of the positive orthant remain unchanged.</p> <p>Let \(\Phi_{\mathcal{GN}_p(0,1)}\) be the cumulative distribution function for the standard Generalized Gaussian distribution \(\mathcal{GN}_p(0, 1)\). In \(d\)-dimensional spaces, the probability of the random vector being in the interior of the positive orthant $[0,\infty)^d$ is $(1-\Phi_{\mathcal{GN}_p(0,1)}(-\mu/\sigma))^d$, which decays to $0$ exponentially fast as $d\to\infty$. Thus in high dimensions, most of the rectified samples concentrates on the boundary of the positive orthant cone.</p> <p>It’s also possible to characterize the probability density function \(f_{\mathcal{RGN}_p(\mu,\sigma)}(\cdot)\) of the univariate Rectified Generalized Gaussian distribution (which we also denote as \(\mathcal{RGN}_p(\mu,\sigma)\)):</p> \[\begin{align} f_{\mathcal{RGN}_p(\mu,\sigma)}(z)&amp;=\Phi_{\mathcal{GN}_p(0,1)}\bigg(-\frac{\mu}{\sigma}\bigg)\cdot\mathbb{1}_{\{0\}}(z)\\&amp;+\frac{p^{1-1/p}}{2\sigma\Gamma(1/p)}\exp\bigg(-\frac{|z-\mu|^p}{p\sigma^p}\bigg)\cdot\mathbb{1}_{(0,\infty)}(z) \end{align}\] <p>where \(\Gamma(\cdot)\) is the Gamm function and \(\mathbb{1}_{S}(z)\) is the indicator function that evaluates to \(1\) if \(z\in S\) and \(0\) otherwise.</p> <h2 id="rectified-distribution-matching-regularization-rdmreg">Rectified Distribution Matching Regularization (RDMReg)</h2> <p>After identifying the desirable target distribution as the Rectified Generalized Gaussian family, we would like to regularize the neural network feature towards it using Eq. (4).</p> <p>Contrary to the isotropic Gaussian, which is closed under linear combinations, the Rectified Generalized Gaussian (RGG) family is not preserved under linear projections: the one-dimensional projected marginals generally fall outside the RGG family. In fact, closure under linear combinations characterizes the class of multivariate stable distributions \citep{nolan1993multivariate}, which is disjoint from our RGG family. As illustrated in \cref{fig:ttt2}, while any linear projection of a Gaussian remains Gaussian, projecting a Rectified Gaussian along different directions yields distinctly different marginals that no longer belong to the Rectified Gaussian family.</p> <p></p> <h2 id="rectified-lpjepa">Rectified LpJEPA</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/final_teasor-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/final_teasor-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/final_teasor-1400.webp"/> <img src="/assets/img/final_teasor.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rectified LpJEPA </div> <div class="l-page"> <div style=" width: 100%; overflow: hidden; height: 480px; /* 800 * 0.6 = 480 */ "> <iframe src="/assets/plotly/vary_mu_p2_sigma1.html" frameborder="0" scrolling="no" width="100%" height="800" style=" border: 1px dashed grey; display: block; transform: scale(0.6); transform-origin: top center; "></iframe> </div> </div> <h2 id="additional-helper">additional helper</h2> <p>You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a></p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> ]]></content><author><name>Yilun Kuang</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations]]></summary></entry></feed>