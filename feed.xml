<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yilunkuang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yilunkuang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-03T05:17:04+00:00</updated><id>https://yilunkuang.github.io/feed.xml</id><title type="html">Yilun Kuang</title><subtitle>Yilun Kuang, Machine Learning, PhD, NYU, AI </subtitle><entry><title type="html">Rectified LpJEPA</title><link href="https://yilunkuang.github.io/blog/2026/rectified-lp-jepa/" rel="alternate" type="text/html" title="Rectified LpJEPA"/><published>2026-01-30T00:00:00+00:00</published><updated>2026-01-30T00:00:00+00:00</updated><id>https://yilunkuang.github.io/blog/2026/rectified-lp-jepa</id><content type="html" xml:base="https://yilunkuang.github.io/blog/2026/rectified-lp-jepa/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post is a companion blog to our paper <strong>Rectified LpJEPA</strong> (<a href="https://arxiv.org/abs/2602.01456">arXiv:2602.01456</a> <i class="ai ai-arxiv" style="font-size:0.95em;"></i>) and its accompanying implementation (<a href="https://github.com/YilunKuang/rectified-lp-jepa">GitHub</a> <i class="fab fa-github" style="font-size:0.95em;"></i>). The goal is to provide a self-contained walkthrough of the key ideas in a less technical manner.</p> <p>We begin by reviewing self-supervised learning and the problem of feature collapse, then motivate <strong>distribution-matching regularization</strong> through the <strong>Cramér–Wold theorem</strong> and its instantiation in <strong>SIGReg / LeJEPA</strong> <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite>. We progressively refine the choice of target distributions—from isotropic Gaussian to Rectified Generalized Gaussian—to highlight the tradeoff between <strong>maximum-entropy</strong> and <strong>sparsity</strong>. Finally, we introduce <strong>Rectified Distribution Matching Regularization (RDMReg)</strong> and the resulting <strong>Rectified LpJEPA</strong> objective. We conclude with a small set of empirical <strong>sparsity–performance tradeoffs</strong>, intended as a representative snapshot of the broader experimental results presented in the paper.</p> <h2 id="self-supervised-learning">Self-Supervised Learning</h2> <p>Consider an input vector $\mathbf{x}$, such as an image, an audio clip, or a video frame. We would like to learn a neural network representation</p> \[\begin{align} \mathbf{z}=f_{\boldsymbol{\theta}}(\mathbf{x})\in\mathbb{R}^d \tag{1} \end{align}\] <p>in the absence of any labeling information, where \(f_{\boldsymbol{\theta}}(\cdot)\) is the neural network with parameters \(\boldsymbol{\theta}\). Self-supervised learning makes this possible by creating supervision from the data itself <d-cite key="balestriero2023cookbookselfsupervisedlearning"></d-cite>. Given $\mathbf{x}$, we can generate another view $\mathbf{x}'$ of the input $\mathbf{x}$ that preserves the same semantic content.</p> <ul> <li>For images, $\mathbf{x}'$ might be a cropped, rotated, or even corrupted version of $\mathbf{x}$</li> <li>For audio or video, $\mathbf{x}'$ can be a nearby time segment or adjacent frame.</li> </ul> <p>Although $\mathbf{x}$ and $\mathbf{x}'$ may look different, they are assumed to be semantically related. Thus we can learn a neural network representation by relying on the following principle: <strong>representations of different views of the same input should be similar</strong>. Translating to mathematical language, this means we can minimize the $\ell_2$ distance between the embeddings of different views</p> \[\begin{align} \min_{\boldsymbol{\theta}}\mathbb{E}_{\mathbf{z},\mathbf{z}'}[\|\mathbf{z}-\mathbf{z}'\|_2] \tag{2} \end{align}\] <p>where \(\mathbf{z}' = f_{\boldsymbol{\theta}}(\mathbf{x}')\) and \(\mathbf{z}'\sim\mathbb{P}_{\mathbf{z}'}\), \(\mathbf{z}\sim\mathbb{P}_{\mathbf{z}}\) are sampled from the feature distributions. By enforcing agreement across many randomly generated views, the network learns features that are invariant to nuisance transformations and capture meaningful structure in the data.</p> <h2 id="distribution-matching-regularization">Distribution-Matching Regularization</h2> <h3 id="distribution-matching-as-regularization">Distribution-Matching as Regularization</h3> <p>Simply minimizing the \(\ell_2\) distance (Eq. (2)) between views, however, can lead to the problem of <strong>feature collapse</strong>. In the extreme case, the network can map every input to the same vector, resulting in <strong>complete collapse</strong>. Eq. (2) is perfectly minimized, but the representation is useless as it cannot distinguish between different inputs. More subtle forms of collapse also occur, such as <strong>dimensional collapse</strong>, where different feature dimensions encode redundant information <d-cite key="jing2022understandingdimensionalcollapsecontrastive"></d-cite>.</p> <p>The goal of self-supervised learning is therefore to enforce invariance across views while maximally spreading feature vectors in the ambient space to prevent collapse. One effective way to do this is to regularize the feature distributions \(\mathbb{P}_{\mathbf{z}}\) and \(\mathbb{P}_{\mathbf{z}'}\) towards a carefully chosen <strong>target distribution</strong> \(Q\), which explicitly encodes desirable properties such as dispersion and diversity across feature dimensions. Thus the self-supervised learning objective in Eq. (2) can be augmented as</p> \[\begin{align} \min_{\boldsymbol{\theta}}\mathbb{E}_{\mathbf{z},\mathbf{z}'}[\|\mathbf{z}-\mathbf{z}'\|_2]+\mathcal{L}(\mathbb{P}_{\mathbf{z}}\|Q)+\mathcal{L}(\mathbb{P}_{\mathbf{z}'}\|Q) \tag{3} \end{align}\] <p>where \(\mathcal{L}(P\|Q)\) is any differentiable distributional discrepancy that is minimized when \(P\) and \(Q\) are equal in distribution.</p> <p>Naively, one can consider the KL divergence \(D_{\operatorname{KL}}(\mathbb{P}_{\mathbf{z}}\|Q)\) with the Monte-Carlo estimate:</p> \[\begin{align} D_{\operatorname{KL}}(\mathbb{P}_{\mathbf{z}}\|Q) = \int\log\frac{d\mathbb{P}_{\mathbf{z}}(\mathbf{z})}{dQ(\mathbf{z})}d\mathbb{P}_{\mathbf{z}}(\mathbf{z})\approx \frac{1}{B}\sum_{i=1}^{B}\log\frac{p_{\mathbf{z}}(\mathbf{z}_i)}{q(\mathbf{z}_i)} \tag{4} \end{align}\] <p>However, directly performing distribution-matching in high dimensional space suffers from the <strong>curse of dimensionality</strong>: density estimations are intractable and we require exponential number of samples in dimensions <d-cite key="mcallester2020formallimitationsmeasurementmutual"></d-cite>. Thus we resort to a family of method based on the Cramér-Wold theorem <d-cite key="cramer1936"></d-cite> <d-cite key="wold1938"></d-cite>.</p> <h3 id="cramér-wold-theorem">Cramér-Wold Theorem</h3> <p>The <strong>Cramér-Wold theorem</strong> states that two random vectors \(\mathbf{x},\mathbf{y}\in\mathbb{R}^d\) are equal in distribution if and only if all of their one-dimensional projected marginals are equal in distribution, i.e.</p> \[\begin{align} \mathbf{x}\stackrel{\operatorname{d}}{=}\mathbf{y}\iff \mathbf{c}^\top\mathbf{x}\stackrel{\operatorname{d}}{=}\mathbf{c}^\top\mathbf{y}\text{ for all }\mathbf{c}\in\mathbb{R}^d \tag{5} \end{align}\] <p>where the superscript \(\operatorname{d}\) above the equal sign denotes equality in distribution. This result enables us to decompose a high-dimensional distribution matching problem into parallelized one-dimension optimizations under many different projections induced by \(\mathbf{c}\), which significantly reduces the sample complexity in each of the one-dimensional problems. This projection-based distribution matching idea traces back to projection pursuit. See <d-cite key="friedman2006projection"></d-cite> for early treatments.</p> <p>With the Cramér–Wold theorem, Eq. (3) can be updated as</p> \[\begin{align} \min_{\boldsymbol{\theta}}\mathbb{E}_{\mathbf{z},\mathbf{z}'}[\|\mathbf{z}-\mathbf{z}'\|_2]+\mathbb{E}_{\mathbf{c}}[\mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})]+\mathbb{E}_{\mathbf{c}}[\mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}'}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})] \tag{6} \end{align}\] <p>where \(\mathbf{y}\sim Q\), \(\mathbf{c}^\top\mathbf{y}\sim\mathbb{P}_{\mathbf{c}^\top\mathbf{y}}\) denotes the distribution of the projected targets, and \(\mathbf{c}^\top\mathbf{z}\sim\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\) represents the distribution of the projected features. Thus we have converted a high-dimensional distribution matching problem \(\mathcal{L}(\mathbb{P}_{\mathbf{z}}\|Q)\) into an expectation over univariate distribution-matching objectives as \(\mathbb{E}_{\mathbf{c}}[\mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})]\).</p> <p>Even if Cramér–Wold theorem guarantees convergence under asymptotic number of projection vectors, in practice we only need finite projections and it suffices to sample the projection vectors \(\mathbf{c}\) uniformly from the unit \(\ell_2\) sphere \(\mathbb{S}^{d-1}_{\ell_2}:=\{\mathbf{x}\in\mathbb{R}^{d}\mid\|\mathbf{x}\|_2=1\}\), rather than from the entire space \(\mathbb{R}^d\) <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite>.</p> <h3 id="sketched-isotropic-gaussian-regularization-sigreg">Sketched Isotropic Gaussian Regularization (SIGReg)</h3> <p>The <strong>SIGReg</strong> objective in LeJEPA <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite> is the first paper which adopts Eq. (6) as the self-supervised learning objective. SIGReg chooses the target distribution \(Q\) to be the isotropic Gaussian distribution \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\), and parameterizes the distribution-matching loss as the <strong>Epps-Pulley</strong> test</p> \[\begin{align} \mathcal{L}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})=\int_{\mathbb{R}}\vert\varphi_{\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}}(t)-\varphi_{\mathbb{P}_{\mathbf{c}^\top\mathbf{y}}}(t)\vert^2 \omega(t)dt \tag{7} \end{align}\] <p>where \(\varphi_{P}\) denotes the characteristic function of the distribution \(P\) and \(\omega(t)=e^{-t^2/2}\) is the weight function. Intuitively, SIGReg minimizes the discrepancy between the characteristic function of the projected features—estimated empirically from minibatch samples—and that of the projected target distribution, which admits a closed-form expression.</p> <p>This formulation yields a <strong>one-sample</strong> goodness-of-fit test: only the feature distribution is estimated from data, while the target distribution is fixed and analytically specified through its characteristic function. As we show later, the <strong>RDMReg</strong> loss for our <strong>Rectified LpJEPA</strong> requires a <strong>two-sample</strong> goodness-of-fit test due to a different choice of the target distribution \(Q\).</p> <h2 id="target-distributions">Target Distributions</h2> <p>In the following section, we discuss choices of the target distribution \(Q\) that encourage maximally spread-out and diverse feature representations, while simultaneously encoding <strong>sparsity</strong>. We begin, however, by revisiting the <strong>isotropic Gaussian</strong>, which induces dense representations and serves as a natural baseline.</p> <h3 id="isotropic-gaussian-distributions">Isotropic Gaussian Distributions</h3> <p>One natural target distribtuion is the <strong>isotropic Gaussian</strong> \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\), which is used in LeJEPA <d-cite key="balestriero2025lejepaprovablescalableselfsupervised"></d-cite>.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-isotropic-gaussian">Probability Density Functions of Isotropic Gaussian</h5> \[p(\mathbf{z})=\frac{1}{(2\pi)^{d/2}}\exp\bigg(-\frac{1}{2}\|\mathbf{z}\|_2^2\bigg)\] </blockquote> <p>The choice of Gaussian as the target distribution is not an arbitrary one. Geometrically, a $d$-dimensional isotropic Gaussian random vector $\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})$ concentrates on a thin shell of radius $\sqrt{d}$ with an $O(1)$ width. Under polar decompositions, the radius $||\mathbf{z}||_2\sim\chi(d)$ and the angular direction $\mathbf{z}/||\mathbf{z}||_2$ are independent, with the direction uniformly distributed over the unit $\ell_2$ sphere with respect to the surface (Hausdorff) measure. This <strong>uniform angular distribution</strong> coincides exactly with the uniformity condition identified as optimal for contrastive learning <d-cite key="wang2022understandingcontrastiverepresentationlearning"></d-cite>. As a result, isotropic Gaussian features are maximally spread out in direction, providing a principled way to prevent feature collapse.</p> <div class="row justify-content-center" style="margin-top: 0.5rem;"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L2_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L2_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" style="margin-top: 0.15rem; margin-bottom: 0.5rem;"> Unit $\ell_2$ Sphere </div> <p>This geometric behavior has a direct information-theoretic correspondence. Among all possible probability distributions with a fixed expected $\ell_2$-norm (i.e., fixed average energy), the isotropic Gaussian <strong>maximizes entropy</strong>. In other words, if we constrain only how much energy the features carry and impose no further structure, the Gaussian is the most “spread out” distribution possible.</p> <h3 id="product-laplace-distributions">Product Laplace Distributions</h3> <p>While isotropic Gaussian regularization effectively prevents feature collapse, it inherently favors dense representations, where most feature dimensions are active. In contrast, extensive evidence from neuroscience, signal processing, and machine learning suggests that <strong>sparse representations</strong> are often more efficient and robust. Sparse coding plays a central role in compressed sensing and robust recovery, and biological neural systems are known to encode sensory inputs using non-negative, sparse activations under metabolic constraints <d-cite key="olshausen1996emergence"></d-cite> <d-cite key="donoho2006compressed"></d-cite> <d-cite key="lee1999learning"></d-cite> <d-cite key="glorot2011deep"></d-cite>.</p> <p>Motivated by these observations, we seek to induce sparsity directly at the level of the feature distribution. A simple and principled approach is to replace isotropic Gaussian regularization with <strong>product Laplace</strong> \(\prod_{i=1}^{d}\mathcal{L}(0,\sigma)\) regularization.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-product-laplace">Probability Density Functions of Product Laplace</h5> \[p(\mathbf{z})=\frac{1}{(2\sigma)^d}\exp\bigg(-\frac{\|\mathbf{z}\|_1}{\sigma}\bigg)\] </blockquote> <p>Contrary to Gaussian, the product Laplace distribution \(\mathbf{z}\sim\prod_{i=1}^{d}\mathcal{L}(0,\sigma)\) is the <strong>maximum-entropy</strong> distribution under a fixed expected $\ell_1$-norm constraint. Its radius follows the Gamma distribution \(\|\mathbf{z}\|_1\sim\Gamma(d/1, 1)\) and the angular direction \(\mathbf{z}/\|\mathbf{z}\|_1\) is <strong>uniformly</strong> distributed over the unit $\ell_1$ sphere with respect to the surface measure.</p> <div class="row justify-content-center" style="margin-top: 0.5rem;"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L1_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L1_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" style="margin-top: 0.15rem; margin-bottom: 0.5rem;"> Unit $\ell_1$ Sphere </div> <p>The geometry of the \(\ell_1\) norm directly explains why the Product Laplace distribution induces sparsity. Unlike the smooth \(\ell_2\) sphere, the \(\ell_1\) geometry has sharp corners along coordinate axes, biasing samples toward configurations where many coordinates are small or close to zero. Thus regularizing feature distributions towards product Laplace lead to axis-aligned, sparse representations, while also encourages maximum spreading and hence prevent feature collapse.</p> <p>The other way to think about why Laplace induces sparsity is through the lens of regularized linear regression. It’s well known that <strong>Lasso regression</strong> with $\ell_1$ penalty on the weight is equivalent to Maximum A Posteriori (MAP) estimation with a <strong>Laplace prior</strong>, whereas <strong>Ridge regression</strong> with $\ell_2$ regularization on the weight corresponds to MAP estimation with a <strong>Gaussian prior</strong> <d-cite key="bishop2006pattern"></d-cite>. Hence just as how the Lasso loss constrains the \(\ell_1\) norm of the parameters, imposing Laplace distributions over features regularizes the output of the neural network functions and thus naturally induces sparsity in the learned representations.</p> <h3 id="generalized-gaussian-distributions">Generalized Gaussian Distributions</h3> <p>We observe that both Laplace and Gaussian are maximum-entropy distributions over either expected \(\ell_1\) amd \(\ell_2\) norm constraints. Since the \(\ell_1\)-norm already promotes sparsity, a natural question is how much further can we go.</p> <p>To answer this, we need to define our sparsity metrics. The most direct notion of sparsity is the <strong>$\ell_0$ (pseudo-)norm</strong> which simply counts the number of nonzero entries in a vector. However, direct minimization of $\ell_0$ norm is an NP-hard problem <d-cite key="natarajan1995sparse"></d-cite>. The <strong>$\ell_1$-norm</strong> is commonly used as a convex surrogate for \(\ell_0\) <d-cite key="tibshirani1996regression"></d-cite>, but it still penalizes all nonzero entries linearly and can produce only approximately sparse solutions.</p> <p>More generally, $\ell_p$ <strong>quasi-norms</strong> with $0 &lt; p &lt; 1$ provide a closer approximation to $\ell_0$. Their sharp singularity near zero strongly encourages exact sparsity, while their weaker growth for large values reduces shrinkage on important components. Although nonconvex, such penalties are well known to yield significantly sparser representations than $\ell_1$ in practice <d-cite key="chartrand2007exact"></d-cite> <d-cite key="chartrand2008iteratively"></d-cite>.</p> <p>Thus we would like to consider distributions with the \(\ell_p\) quasi-norms constraints. In fact, the <strong>maximum-entropy</strong> distribution under the expected \(\ell_p\)-norm constraints is the zero-mean <strong>product Generalized Gaussian</strong> distributions \(\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\), of which product Laplace and isotropic Gaussian are special cases for \(p=1\) and \(p=2\) respectively.</p> <blockquote class="block-tip"> <h5 id="probability-density-functions-of-product-generalized-gaussian">Probability Density Functions of Product Generalized Gaussian</h5> \[\begin{aligned} p(\mathbf{z}) &amp;= \prod_{i=1}^{d}\frac{p^{1-1/p}}{2\sigma\Gamma(1/p)} \exp\!\left(-\frac{|\mathbf{z}_i-\mu|^p}{p\sigma^p}\right) \\ &amp;= \frac{p^{d-d/p}}{(2\sigma)^d\Gamma(1/p)^d} \exp\!\left(-\frac{\|\mathbf{z}-\boldsymbol{\mu}\|_p^p}{p\sigma^p}\right) \end{aligned}\] </blockquote> <p>Assume that $\mu=0$ and let \(\mathbf{z}\sim\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\). Then the radius \(r^p:=\|\mathbf{z}\|_p^p\sim\Gamma(d/p,p\sigma^p)\) follows the Gamma distribution and the angular direction \(\mathbf{u}:=\mathbf{z}/\|\mathbf{z}\|_p\) follows the cone measure on the $\ell_p$ sphere \(\mathbb{S}^{d-1}_{\ell_{p}}:=\{\mathbf{z}\in\mathbb{R}^d\mid\|\mathbf{z}\|_p=1\}\) with the radial-angular independence \(r\perp \!\!\ \mathbf{u}\) <d-cite key="barthe2005probabilistic"></d-cite>.</p> <details><summary>Details on Cone and Surface Measure</summary> <p>The cone measure is identical to the \((d-1)\)-dimensional Hausdorff measure \(\mathcal{H}^{d-1}\) (also called surface measure) when \(p\in\{1,2,\infty\}\) <d-cite key="alonso2019gaussian"></d-cite>. By definition, if \(A\subseteq \mathbb{S}^{d-1}_{\ell_{p}}\), then \(p(\mathbf{u}\in A)=\mathcal{H}^{d-1}(A)/\mathcal{H}^{d-1}(\mathbb{S}^{d-1}_{\ell_{p}})\).</p> <p>Thus the angular directions of product Laplace and isotropic Gaussian are uniformly distributed over the \(\ell_p\) sphere with respect to the surface measures, whereas any other Generalized Gaussian distributions have angular direction uniform under the cone measure.</p> </details> <div class="row mt-3 text-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.75_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.75_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.75}$ sphere </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.50_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.50_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.5}$ sphere </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/L0.25_sphere_full-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/L0.25_sphere_full.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Unit $\ell_{0.25}$ sphere </div> </div> </div> <p>Thus we can always regularizes our feature distributions towards the Generalized Gaussian Distributions \(\prod_{i=1}^{d}\mathcal{GN}_p(\mu,\sigma)\) with \(0&lt;p&lt;1\) for learning even sparser, axis-aligned representations while also preserving the maximum-entropy guarantee to prevent feature collapse.</p> <p>We denote the family of methods using Eq. (6) with the target distributions being Generalized Gaussian \(\prod_{i=1}^{d}\mathcal{GN}_p(0,\sigma)\) as <strong>LpJEPA</strong>. When \(p=2\), LpJEPA reduce to LeJEPA since the target distribution becomes the isotropic Gaussian.</p> <h3 id="rectified-generalized-gaussian-distributions-rgg">Rectified Generalized Gaussian Distributions (RGG)</h3> <p>The Generalized Gaussian family is a well-known distribution, but we’re not satisfied with the \(\ell_p\)-norm sparsity it induces. In fact, it’s possible to directly encode \(\ell_0\)-norm into the target distribution, and this brings us to the key innovation of our paper: <strong>regularizing rectified features towards the Rectified Generalized Gaussian distributions</strong>.</p> <p>Let \(\mathbf{x}\sim\prod_{i=1}^d\mathcal{GN}_p(\mu,\sigma)\) be a Generalized Gaussian random vector. Then we can obtain the (product) Rectified Generalized Gaussian random vector as \(\mathbf{z}\sim\prod_{i=1}^d\operatorname{ReLU}(\mathcal{GN}_p(\mu,\sigma))\), where we apply the rectifying nonlinearities coordinate-wise to the Generalized Gaussian random vector. We visualize the samples drawn from the Generalized Gaussian and Rectified Generalized Gaussian distribution in \(2\)-dimensions when \(p=2\).</p> <div class="row mt-3 text-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_False.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Samples from Gaussian and Rectified Gaussian </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/gaussian_vs_rectified_scatter_True.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> ReLU sqaushes the Gaussian samples into the axis. </div> </div> </div> <p>As illustrated in the figure above, rectification collapses all samples lying outside the positive orthant onto its boundary, while samples in the interior of the positive orthant remain unchanged.</p> <p>Let \(\Phi_{\mathcal{GN}_p(0,1)}\) be the cumulative distribution function for the standard Generalized Gaussian distribution \(\mathcal{GN}_p(0, 1)\). In \(d\)-dimensional spaces, the probability of a Rectified Generalized Gaussian random vector being in the interior of the positive orthant $[0,\infty)^d$ is $(1-\Phi_{\mathcal{GN}_p(0,1)}(-\mu/\sigma))^d$, which decays to $0$ exponentially fast as $d\to\infty$. Thus in high dimensions, most of the rectified samples concentrates on the boundary of the positive orthant cone.</p> <p>It’s also possible to characterize the probability density function \(f_{\mathcal{RGN}_p(\mu,\sigma)}(\cdot)\) of the univariate Rectified Generalized Gaussian distribution (which we also denote as \(\mathcal{RGN}_p(\mu,\sigma)\)):</p> \[\begin{align} f_{\mathcal{RGN}_p(\mu,\sigma)}(z)&amp;=\Phi_{\mathcal{GN}_p(0,1)}\bigg(-\frac{\mu}{\sigma}\bigg)\cdot\mathbb{1}_{\{0\}}(z)\tag{8}\\&amp;+\frac{p^{1-1/p}}{2\sigma\Gamma(1/p)}\exp\bigg(-\frac{|z-\mu|^p}{p\sigma^p}\bigg)\cdot\mathbb{1}_{(0,\infty)}(z)\tag{9} \end{align}\] <p>where \(\Gamma(\cdot)\) is the Gamm function and \(\mathbb{1}_{S}(z)\) is the indicator function that evaluates to \(1\) if \(z\in S\) and \(0\) otherwise. We note the the probability density function here is in fact the Radon-Nikodym derivative of the Rectified Generalized Gaussian probability measure with respect to the Dirac + Lebesgue measure. Further details can be found in the paper.</p> <details><summary>Measure-Theoretical Characterizations of the Rectified Generalized Gaussian Distribution</summary> <p>Fix parameters \(p&gt;0\), \(\mu\in\mathbb{R}\), and \(\sigma&gt;0\). We denote \((\mathbb{R}, \mathcal{B}(\mathbb{R}))\) as the real line equipped with Borel \(\sigma\)-algebra. Let \(\lambda\) be the Lebesgue measure on \(\mathcal{B}(\mathbb{R})\) and let \(\delta_0\) be the Dirac measure at \(0\). The probability measure \(\mathbb{P}_{X}\) of the Rectified Generalized Gaussian random variable \(X\) is given by the mixture</p> \[\mathbb{P}_{X}=\Phi_{\mathcal{GN}_p(0,1)}\bigg(-\frac{\mu}{\sigma}\bigg)\cdot\delta_0+\bigg(1-\Phi_{\mathcal{GN}_p(0,1)}\bigg(-\frac{\mu}{\sigma}\bigg)\bigg)\cdot\mathbb{P}_{\mathcal{TGN}_p(\mu,\sigma)}\] <p>where \(\mathbb{P}_{\mathcal{TGN}_p(\mu,\sigma)}\) is the Truncated Generalized Gaussian probability measure and \(\Phi_{\mathcal{GN}_p(0,1)}\) is the CDF of the standard Generalized Gaussian \(\mathcal{GN}_{p}(0,1)\).</p> <p>Define the mixed measure \(\nu:=\lambda+\delta_0\). The Radon-Nikodym derivative of \(\mathbb{P}_X\) with respect to \(\nu\) exists and is given by</p> \[\frac{d\mathbb{P}_{X}}{d\nu}(x)=f_{\mathcal{RGN}_p(\mu,\sigma)}(x)\] </details> <p>Intuitively, the Rectified Generalized Gaussian distribution can also be viewed as a mixture between a discrete Dirac measure \(\delta_0(x)\) and a Truncated Generalized Gaussian distribution \(\mathcal{TGN}_{p}(\mu,\sigma,(0,\infty))\), where \(\mathcal{TGN}_{p}(\mu,\sigma,(0,\infty))\) has the density function</p> \[\begin{align} f_{\mathcal{TGN}_{p}(\mu, \sigma, (0,\infty))}(z)=\frac{\mathbb{1}_{(0,\infty)}(z)}{Z_S(\mu,\sigma,p)}\exp\bigg(-\frac{|z-\mu|^p}{p\sigma^p}\bigg) \tag{10} \end{align}\] <h3 id="sparsity-and-entropy-characterizations-of-rgg">Sparsity and Entropy Characterizations of RGG</h3> <p>In our paper, we show that if \(\mathbf{x}\sim\prod_{i=1}^{d}\mathcal{RGN}_p(\mu,\sigma)\) in \(d\) dimension, then</p> \[\begin{align} \mathbb{E}[\|\mathbf{x}\|_0]&amp;=d\cdot\Phi_{\mathcal{GN}_p(0,1)}\bigg(\frac{\mu}{\sigma}\bigg)\tag{11}\\ &amp;=\frac{d}{2}\bigg(1+\operatorname{sgn}\bigg(\frac{\mu}{\sigma}\bigg)P\bigg(\frac{1}{p},\frac{|\mu/\sigma|^p}{p}\bigg)\bigg)\tag{12} \end{align}\] <p>where \(\operatorname{sgn}(\cdot)\) is the sign function and \(P(\cdot,\cdot)\) is the lower regularized gamma function. Thus it’s possible to directly control the expected \(\ell_0\) norm of the Rectified Generalized Gaussian random vector through specifying the set of parameters \(\{\mu,\sigma,p\}\).</p> <p>Due to rectifications, the RGG distribution is also no longer absolutely continuous with respect to the Lebesgue measure, rendering differential entropy ill-defined. Thus we resort to the concept of \(d(\boldsymbol{\xi})\)-dimensional entropy <d-cite key="renyi1959dimension"></d-cite>, which measures the Shannon entropy of quantized random vector under successive grid refinement. We are able to show that the Rectified Generalized Gaussian distribution preserves maximal-entropy guarantees—rescaled by the Rényi information dimension—while explicitly inducing \(\ell_0\) sparsity.</p> <details><summary>Details on \(d(\boldsymbol{\xi})\)-dimensional entropy</summary> <p>Let \(\boldsymbol{\xi}\sim\prod_{i=1}^{D}\mathcal{RGN}_p(\mu,\sigma)\) be a Rectified Generalized Gaussian random vector. The Rényi information dimension of \(\boldsymbol{\xi}\) is \(d(\boldsymbol{\xi})=D\cdot\Phi_{\mathcal{GN}_p(0,1)}(\mu/\sigma)\), and the \(d(\boldsymbol{\xi})\)-dimensional entropy of \(\boldsymbol{\xi}\) is given by</p> \[\begin{align} \mathbb{H}_{d(\boldsymbol{\xi}_i)}(\boldsymbol{\xi}_i)&amp;=\Phi_{\mathcal{GN}_p(0,1)}\bigg(\frac{\mu}{\sigma}\bigg)\cdot\mathbb{H}_{1}(\mathcal{TGN}_p(\mu,\sigma))\tag{*}\\ &amp;+\mathbb{H}_{0}(\mathbb{1}_{(0,\infty)}(\boldsymbol{\xi}_i))\tag{*}\\ \mathbb{H}_{d(\boldsymbol{\xi})}(\boldsymbol{\xi})&amp;=\sum_{i=1}^{D}\mathbb{H}_{d(\boldsymbol{\xi}_i)}(\boldsymbol{\xi}_i)=D\cdot \mathbb{H}_{d(\boldsymbol{\xi}_i)}(\boldsymbol{\xi}_i)\tag{*} \end{align}\] <p>where \(\mathbb{H}_0(\cdot)\) is the discrete Shannon entropy, \(\mathbb{H}_1(\cdot)\) denotes the differential entropy, and \(\mathbb{1}_{(0,\infty)}(\boldsymbol{\xi}_i)\) is a Bernoulli random variable that equals \(1\) with probability \(\Phi_{\mathcal{GN}_p(0,1)}(\mu/\sigma)\) and \(0\) with probability \(1-\Phi_{\mathcal{GN}_p(0,1)}(\mu/\sigma)\).</p> </details> <p>Thus we choose the Rectified Generalized Gaussian as our target distribution \(Q\), which has an explicit expected \(\ell_0\) norm guarantees, while also preserve the maximum-entropy property under the expected \(\ell_p\) norm constraint under rescaling. The distribution-matching loss towards the Rectified Generalized Gaussian family is called <strong>Rectified Distribution Matching Regularization (RDMReg)</strong>, and a Joint-Embedding Predictive Architecture (JEPA) equipped with RDMReg is called <strong>Rectified LpJEPA</strong>. We dive into these two things in the rest of this blog post.</p> <h2 id="rectified-distribution-matching-regularization-rdmreg">Rectified Distribution Matching Regularization (RDMReg)</h2> <p>After identifying the desirable target distribution as the Rectified Generalized Gaussian family, we would like to regularize the neural network feature towards it using Eq. (6).</p> <p>Contrary to the isotropic Gaussian, which is closed under linear combinations, the Rectified Generalized Gaussian (RGG) family is not preserved under linear projections: the one-dimensional projected marginals generally fall outside the RGG family. In fact, <strong>closure under linear combinations</strong> characterizes the class of multivariate stable distributions <d-cite key="nolan1993multivariate"></d-cite>, which includes Gaussian but is disjoint from our RGG family. As illustrated in the following figure, while any linear projection of a Gaussian remains Gaussian, projecting a Rectified Gaussian along different directions yields distinctly different marginals that <strong>no longer belong to the Rectified Gaussian family.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/rectified_gaussian_shadow_grid_8x8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/rectified_gaussian_shadow_grid_8x8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/rectified_gaussian_shadow_grid_8x8-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/rectified_gaussian_shadow_grid_8x8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Projected Marginals of 2D Rectified Gaussian exhibit distinct patterns in different directions. </div> <p>Consequently, the distribution matching loss \(\mathcal{L}(\cdot\|\cdot)\) must rely on sample-based, nonparametric two-sample hypothesis tests on projected marginals <d-cite key="lehmann1951consistency"></d-cite>. Among many possible choices, we instantiate this objective using the sliced \(2\)-Wasserstein distance <d-cite key="bonneel2015sliced"></d-cite> <d-cite key="kolouri2018swae"></d-cite>.</p> <p>Let \(\mathbf{Z},\mathbf{Y}\in\mathbb{R}^{B\times D}\) be empirical neural network feature matrix and the samples from RGG where \(B\) is batch size and \(D\) is dimension. We denote a single random projection vector as \(\mathbf{c}_i\in\mathbb{R}^D\) out of \(N\) total projections.</p> <p>The <strong>Rectified Distribution Matching Regularization (RDMReg)</strong> loss function is given by</p> \[\begin{align} \mathcal{L}_{\operatorname{RDMReg}}(\mathbb{P}_{\mathbf{c}_i^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}_i^\top\mathbf{y}}):=\frac{1}{B}\|(\mathbf{Z}\mathbf{c}_i)^{\uparrow}-(\mathbf{Y}\mathbf{c}_i)^{\uparrow}\|_2^2\tag{13} \end{align}\] <p>where \((\cdot)^{\uparrow}\) denotes sorting in ascending order. The objective is implemented in the following python code snippet</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">rdmreg_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">target_samples</span><span class="p">,</span> <span class="n">num_projections</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">device</span>
    
    <span class="c1"># 1. Sample random projections from the unit L2 sphere.
</span>    <span class="n">projections</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_projections</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">projections</span> <span class="o">=</span> <span class="n">projections</span> <span class="o">/</span> <span class="n">projections</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># 2. Project features and samples from the RGG distribution.
</span>    <span class="n">proj_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">projections</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">proj_target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">target_samples</span><span class="p">,</span> <span class="n">projections</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    
    <span class="c1"># 3. Sort along the batch dimension
</span>    <span class="n">proj_z_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">proj_z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">proj_target_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">proj_target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># 4. Compute and Return the Sliced 2-Wasserstein distance
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">proj_z_sorted</span> <span class="o">-</span> <span class="n">proj_target_sorted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></code></pre></figure> <h2 id="rectified-lpjepa">Rectified LpJEPA</h2> <p>Equipped with <strong>RDMReg</strong>, we arrive at our final objective function</p> \[\begin{align} \min_{\boldsymbol{\theta}}\mathbb{E}_{\mathbf{z},\mathbf{z}'}[\|\mathbf{z}-\mathbf{z}'\|_2]&amp;+\mathbb{E}_{\mathbf{c}}[\mathcal{L}_{\operatorname{RDMReg}}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})]\tag{14}\\&amp;+\mathbb{E}_{\mathbf{c}}[\mathcal{L}_{\operatorname{RDMReg}}(\mathbb{P}_{\mathbf{c}^\top\mathbf{z}'}\|\mathbb{P}_{\mathbf{c}^\top\mathbf{y}})]\tag{15} \end{align}\] <p>where \(\mathbf{y}\sim\prod_{i=1}^{d}\mathcal{RGN}_{p}(\mu,\sigma)\) is a Rectified Generalized Gaussian random vector and \(\mathbf{z}=f_{\boldsymbol{\theta}}(\mathbf{x})\), \(\mathbf{z}'=f_{\boldsymbol{\theta}}(\mathbf{x}')\) are the neural network features.</p> <p>In standard self-supervised learning protocols <d-cite key="balestriero2023cookbookselfsupervisedlearning"></d-cite>, representations are typically parameterized as \(\mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x}) = g_{\boldsymbol{\theta}_2}\bigl(g_{\boldsymbol{\theta}_1}(\mathbf{x})\bigr),\) where \(g_{\boldsymbol{\theta}_1}\) denotes a backbone network such as ResNet <d-cite key="he2016deep"></d-cite> or ViT <d-cite key="dosovitskiy2020image"></d-cite>, and \(g_{\boldsymbol{\theta}_2}\) is an additional multilayer perceptron (MLP).</p> <p>In contrast, we explicitly introduce an additional rectification at the output and we arrive at: \(\mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x})= \operatorname{ReLU}\!\left(g_{\boldsymbol{\theta}_2}\bigl(g_{\boldsymbol{\theta}_1}(\mathbf{x})\bigr)\right).\) This final \(\operatorname{ReLU}(\cdot)\) is a deliberate architectural choice and is essential for the correctness of our method. Combining the architectural and objective-level updates, we arrive at <strong>Rectified LpJEPA</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/final_teasor-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/final_teasor-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/final_teasor-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/final_teasor.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rectified LpJEPA Diagram </div> <h2 id="sparsity-performance-tradeoffs">Sparsity-Performance Tradeoffs</h2> <p>Finally, we can train our Rectified LpJEPA model. For simplicity, we can consider self-supervised pretraining over the CIFAR-100 dataset. By varying the mean shift values \(\mu\) and the \(\ell_p\) parameter \(p\) while fixing \(\sigma=\Gamma(1/p)^{1/2}/(p^{1/p}\cdot\Gamma(3/p)^{1/2})\), we observe a clear sparsity-performance tradeoff</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rectified_lp_jepa/l0_norm_sparsity_vs_accuracy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rectified_lp_jepa/l0_norm_sparsity_vs_accuracy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rectified_lp_jepa/l0_norm_sparsity_vs_accuracy-1400.webp"/> <img src="/assets/img/rectified_lp_jepa/l0_norm_sparsity_vs_accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sparsity-Performance Tradeoffs </div> <p>More experiments and ablation studies can be found in the paper. Overall, <strong>Rectified LpJEPA</strong> provides a principled approach to learning sparse representations through distribution matching to Rectified Generalized Gaussian. If you find this work useful, please consider checking out the paper and codebase and citing our work:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{kuang2026rectifiedlpjepajointembeddingpredictive,
      title={Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations}, 
      author={Yilun Kuang and Yash Dagade and Tim G. J. Rudner and Randall Balestriero and Yann LeCun},
      year={2026},
      eprint={2602.01456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2602.01456}, 
}
</code></pre></div></div> ]]></content><author><name>Yilun Kuang</name></author><category term="JEPA"/><category term="Sparsity"/><summary type="html"><![CDATA[Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations]]></summary></entry></feed>